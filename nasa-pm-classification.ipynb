{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Predictive Maintenance using an RNN with Watson Studio\n",
    "   \n",
    "Machine learning models for predictive maintenance predict equipment failure before it happens avoiding unplanned downtime costs resulting from failure and avoiding in some cases,  hundreds of thousands of dollars per day depending on industry.\n",
    "\n",
    "Most equipment will have sensors that generate time series data that can be used to build a machine learning model to predict failure. There are different techniques to building model with this kind of data.\n",
    "\n",
    "This Watson Studio lab  will demonstrate one such technique building a classification model using an  RNN with LSTM to predict machine failure within a specific time horizon (e.g. the next 10 days). RNNs work well with time series data as they can ingest sequences of data and find predictive capability in these ordered sequences (as opposed to models that just ingest unordered pieces of data).  The data used to train the model comes from NASA and was released to the general public. It has testing and training data that includes sensor data for aircraft engines and failure data for each engine in a time series. It was downloaded from [this NASA website](https://c3.nasa.gov/dashlink/resources/139/)\n",
    "\n",
    "## Setup\n",
    "    \n",
    "1. Download the file with the NASA data from [here](https://raw.githubusercontent.com/ibm-ai-education/predictive-maintenance-classification-lab/master/data/nasa-pm-data.zip) to your local system. The name of the file is nasa-pm-data.zip\n",
    "\n",
    "2. Unzip the file in an empty folder on your system\n",
    "  \n",
    "3. Click on the data icon  at the top right of the notebook window and then select and upload the following 3 files one by one\n",
    "\n",
    "  * train_FD001.csv\n",
    "  * test_FD001.csv\n",
    "  * RUL_FD001.csv\n",
    "\n",
    "![Data icon](https://raw.githubusercontent.com/ibm-ai-education/predictive-maintenance-classification-lab/master/images/ss6.png) \n",
    "\n",
    "    \n",
    "4. Once the files are uploaded, run each cell in the notebook after reading the description of what is being done with each cell.  For cells that instruct you to insert code to create a Dataframe for a file, put the cursor in the cell and then select the following from the data area on the right \n",
    "\n",
    "      **Insert to code->Insert pandas Dataframe**\n",
    "\n",
    "![Insert code](https://raw.githubusercontent.com/ibm-ai-education/predictive-maintenance-classification-lab/master/images/ss7.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With your cursor in this cell, insert the code to read the train_FD001.csv dataset into a DataFrame as instructed in \n",
    "# Step 4) in the 1st  cell of this notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: df_train must be set to  the variable created for the dataframe in the cell abaove. \n",
    "df_train = df_data_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With your cursor in this cell, insert the code to read the test_FD001.csv dataset into a DataFrame as instructed in\n",
    "# Step 4) in the 1st  cell of this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: df_test must be set to  the variable created for the dataframe in the cell abaove.\n",
    "df_test = df_data_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With your cursor in this cell, insert the code to read the RUL_FD001.csv dataset into a DataFrame as instructed in\n",
    "# Step 4) in the 1st  cell of this notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: df_test_ground_truth must be set to  the variable created for the dataframe in the cell abaove.\n",
    "df_test_ground_truth = df_data_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare training data\n",
    "We need to add a column to the training data that indicates the time to failure  of each row and a column indicating whether or not the time to failure is less than or equal to our time horizon (ie 10 time periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our time horizon to predict failure\n",
    "time_horizon = 10\n",
    "# Add time to failure column\n",
    "df_train['ttf'] = df_train.groupby(['engine_id'])['elapsed_time'].transform(max)-df_train['elapsed_time']\n",
    "# Add label indicating  failure within our time horizon\n",
    "df_train['failed_within_time_horizon'] = df_train['ttf'].apply(lambda x: 1 if x <= time_horizon else 0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data\n",
    "We need to add a column to the test  data that indicates the time to failure of each row and a column indicating whether or not the time to failure is less than or equal to our time horizon (ie 20 time periods). This is a bit more complicated than doing this for the training data as the failure information for the test data is in a separate dataframe (ie `df_test_ground_truth`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last recorded time in the test data for each machine\n",
    "lrt = pd.DataFrame(df_test.groupby('engine_id')['elapsed_time'].max()).reset_index()\n",
    "lrt.columns = ['engine_id', 'last_recorded_time']\n",
    "lrt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual time of failure for test data \n",
    "df_test_ground_truth['time_of_failure']=df_test_ground_truth['time_to_failure'] + lrt['last_recorded_time']\n",
    "df_test_ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ground truth data into test data and calculate time to failure\n",
    "df_test_ground_truth.drop('time_to_failure', axis=1, inplace=True)\n",
    "df_test=df_test.merge(df_test_ground_truth,on=['engine_id'],how='left')\n",
    "df_test['ttf']=df_test['time_of_failure'] - df_test['elapsed_time']\n",
    "df_test.drop('time_of_failure', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Add label indicating  failure within our time horizon\n",
    "df_test['failed_within_time_horizon'] = df_test['ttf'].apply(lambda x: 1 if x <= time_horizon else 0)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for LSTM\n",
    "LSTM requires the training and testing data in the form of an array of sequences.  We'll also normalize the  feature values to prevent large  data values from  unduly influencing our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for convenience\n",
    "feature_columns = ['elapsed_time_norm','setting1','setting2','setting3','sensor1','sensor2','sensor3','sensor4','sensor5','sensor6','sensor7','sensor8','sensor9','sensor10','sensor11','sensor12','sensor13','sensor14','sensor15','sensor16','sensor17','sensor18','sensor19','sensor20','sensor21']\n",
    "target_column = 'failed_within_time_horizon'\n",
    "key_columns = ['engine_id','elapsed_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training and testing data\n",
    "#scaler=StandardScaler()\n",
    "scaler=MinMaxScaler()\n",
    "df_train_scaled = df_train.copy()\n",
    "df_test_scaled = df_test.copy()\n",
    "\n",
    "df_train_scaled['elapsed_time_norm'] = df_train_scaled['elapsed_time']\n",
    "df_test_scaled['elapsed_time_norm'] = df_test_scaled['elapsed_time']\n",
    "df_train_scaled[feature_columns]=scaler.fit_transform(df_train_scaled[feature_columns])\n",
    "df_test_scaled[feature_columns]=scaler.transform(df_test_scaled[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to reshape the training and testing data for LSTM\n",
    "def gen_sequence(input_df, seq_length, seq_cols):\n",
    "    df_zeros=pd.DataFrame(np.zeros((seq_length-1,input_df.shape[1])),columns=input_df.columns)\n",
    "    input_df=df_zeros.append(input_df,ignore_index=True)\n",
    "    data_array = input_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    lstm_array=[]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        lstm_array.append(data_array[start:stop, :])\n",
    "    return np.array(lstm_array)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_label(input_df, seq_length, seq_cols,label):\n",
    "    df_zeros=pd.DataFrame(np.zeros((seq_length-1,input_df.shape[1])),columns=input_df.columns)\n",
    "    input_df=df_zeros.append(input_df,ignore_index=True)\n",
    "    data_array = input_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    y_label=[]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        y_label.append(input_df[label][stop])\n",
    "    return np.array(y_label)\n",
    "\n",
    "# function to generate key mapping from generated data to original data\n",
    "def gen_keymap(input_df, seq_length, keys):\n",
    "    df_zeros=pd.DataFrame(np.zeros((seq_length-1,input_df.shape[1])),columns=input_df.columns)\n",
    "    input_df=df_zeros.append(input_df,ignore_index=True)\n",
    "    data_array = input_df[keys].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    y_keys=[]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        y_keys.append([input_df[keys[0]][stop], input_df[keys[1]][stop]])\n",
    "    return np.array(y_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data using the reshaping functions\n",
    "sequence_length = 50\n",
    "x_train=np.concatenate(list(list(gen_sequence(df_train_scaled[df_train_scaled['engine_id']==id], sequence_length, feature_columns)) for id in df_train_scaled['engine_id'].unique()))\n",
    "print(x_train.shape)\n",
    "\n",
    "# generate y_train\n",
    "y_train=np.concatenate(list(list(gen_label(df_train_scaled[df_train_scaled['engine_id']==id], sequence_length, feature_columns,target_column)) for id in df_train_scaled['engine_id'].unique()))\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data using the reshaping functions\n",
    "x_test=np.concatenate(list(list(gen_sequence(df_test_scaled[df_test_scaled['engine_id']==id], sequence_length, feature_columns)) for id in df_test_scaled['engine_id'].unique()))\n",
    "print(x_test.shape)\n",
    "\n",
    "# generate y_test\n",
    "y_test=np.concatenate(list(list(gen_label(df_test_scaled[df_test_scaled['engine_id']==id], sequence_length, feature_columns,target_column)) for id in df_test_scaled['engine_id'].unique()))\n",
    "print(y_test.shape)\n",
    "\n",
    "# Generate keymap to map reshaped test data to original test data\n",
    "x_test_keymap=np.concatenate(list(list(gen_keymap(df_test_scaled[df_test_scaled['engine_id']==id], sequence_length, key_columns)) for id in df_test_scaled['engine_id'].unique()))\n",
    "print(x_test_keymap.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use weights because there are relatively few failures in the dataset. Weights allow the cost function \n",
    "# to penalize wrong predictions for the  sparse label more. It;s imporatnt to do this when false negatives\n",
    "# cost the organization more than false positives \n",
    "class_weights =  dict(enumerate(class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM Model\n",
    "We'll build the LSTM model using Keras. \n",
    "We use two LSTM layers each followwed by a Droput layer (to avoid overfitting) followed by a Dense layer that uses sigmoid activation (because we've framed  this as a Classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model in Keras\n",
    "number_of_features = x_train.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, number_of_features),\n",
    "         units=2*sequence_length,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=sequence_length,\n",
    "          return_sequences=False))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model to Training Data\n",
    "Next we'll apply the model to our training data. Note this step  will take about 10 minutes to complete. Now is a good time to grap some coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with training data\n",
    "history=model.fit(x_train, y_train, epochs=10, batch_size=4*sequence_length, validation_split=0.30, verbose=1, class_weight=class_weights,\n",
    "     callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')])\n",
    "#history=model.fit(x_train, y_train, epochs=10, batch_size=4*sequence_length, validation_split=0.30, verbose=1, \n",
    "#        callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze model and run with test data\n",
    "Here we'll analyze the model's performance. We'll look at both train9ng and validation data by epaoch to see if the model appears to converge over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation and training loss vs epoch number\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training metrics\n",
    "scores = model.evaluate(x_train, y_train, verbose=1, batch_size=200)\n",
    "print('Accuracy: {}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test data through model\n",
    "We'll look at how the modle the test data set (data it has never seen before to assess how well the model generalizes to data outside the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test data through model, compute  accuuracy and print a confusion matrix. \n",
    "y_pred=model.predict_classes(x_test)\n",
    "print('Accuracy of model on test data: ',accuracy_score(y_test,y_pred))\n",
    "print('Confusion Matrix: \\n',confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Normally for predictive maintenance the most important value in the confusion matrix is the nunber at the bottom left. This is the number of false negatives which represents machine failure that is incorrectly predicted as not failing during the time horizon. The number at the top right is the number of false positives. This represents a prediction of failure from the model when there was not a failure. This tends to be less costlty because it would lead to premature maintenance on a piece of equipment  that  would evertually be serviced anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See which engines we missed our predictions for \n",
    "\n",
    "We'll generate a list of indexes for false postivies and false negatives and map them back to the test data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get false positives and false positives\n",
    "y_pred_flat = y_pred.reshape(-1)\n",
    "false_positives = np.flatnonzero(np.asarray(y_pred_flat>y_test)).tolist()\n",
    "false_negatives = np.flatnonzero(np.asarray(y_pred_flat<y_test)).tolist()  \n",
    "print(f\"{len(false_positives)} false positives\")\n",
    "print(f\"{len(false_negatives)} false negatives\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the details of the  false positives are displayed. The ttf column shows the impact  of following the model's predictions. Subtract 10 from this number to see how much earlier  the model is telling us to service the machines in this list. If the  value is 16 for example it means our model causes us to service that machine 6 time units earlier than we actually need to. This is typically cheaper than false negatives where the model fails to predict machine failure before they actually occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positives\n",
    "# Copy structure of df_test but not data\n",
    "df_false_positives = df_test[0:0]\n",
    "# Use the generated keymap for the test data to look up corresponding data in test data frame\n",
    "for i in range(len(false_positives)):\n",
    "    df_false_positives = df_false_positives.append(df_test[(df_test[key_columns[0]] == x_test_keymap[false_positives[i]][0]) & (df_test[key_columns[1]] == x_test_keymap[false_positives[i]][1])])\n",
    "df_false_positives.head(df_false_positives.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negatives\n",
    "# Copy structure of df_test but not data\n",
    "df_false_negatives = df_test[0:0]\n",
    "# Use the generated keymap for the test data to look up corresponding data in test data frame\n",
    "for i in range(len(false_negatives)):\n",
    "    df_false_negatives = df_false_negatives.append(df_test[(df_test[key_columns[0]] == x_test_keymap[false_negatives[i]][0]) & (df_test[key_columns[1]] == x_test_keymap[false_negatives[i]][1])])\n",
    "df_false_negatives.head(df_false_negatives.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Congratulations ! You've gone through an example of using a Recurrent Neural Network to build a predictive maintenance model framed as a classification problem where we try to predict which pieces of equipment will fail in a given time horizon. There are other approaches to predictive  maintenance like using regression to predict the remaining useful life (RUL) of a piece of equipmnet or  using anomaly detection to quickly identify outlier sensor  data that is correlated with machine failure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
